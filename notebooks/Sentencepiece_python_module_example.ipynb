{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9BDzLVkUFT4"
      },
      "source": [
        "# Sentencepiece python module\n",
        "\n",
        "\n",
        "This notebook describes comprehensive examples of sentencepiece Python module.\n",
        "Since Python module calls C++ API through SWIG,  this document is also useful for developing C++ client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgXb6P2Yg6g"
      },
      "source": [
        "## Install and data preparation\n",
        "\n",
        "We use the small training data (TIKVAH.txt) in this example.\n",
        "([TIKVAH-ETHIOPIA](https://t.me/tikvahethiopia) is a Telegram channel having over 1.2M subscribers.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5KbVgiYae-"
      },
      "source": [
        "## Basic  end-to-end example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9W6wGnVteW",
        "outputId": "887ee49f-b08c-4acb-a731-5f95a54d0131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n",
            "[688, 231, 1062, 1776]\n",
            "_በአዲስ_አበባ_የአሜሪካ_ኤምባሲ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=cleaned.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: cleaned.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: cleaned.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5053 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: መልሱ ቀድሞ የመለሰው ●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደ���ጃቸው 1⃣ = 2 1⃣ =1 1⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 1⃣ = 1 1⃣   = 1 1⃣ §ä = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ = 2 2⃣ =1 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 2⃣ = 1 2⃣   = 1 2⃣ §ä = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ = 2 2⃣ ß =1 2⃣ =1 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 2⃣ = 1 2⃣   = 1 2⃣ §ä = 1 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ =6 2���●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3 2⃣ =3 2⃣  ⚽⚽ =3 2⃣ =3 2⃣ =3 2⃣ =3 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3 2⃣ =3 2⃣  ⚽⚽ =3 2⃣ =3 2⃣ =3 2⃣ =3 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️የአለማችን የመጀመሪያዋ መኪና የ��ሰራችው እንደ ፈረንጆቹ አቆጣጠር በ1885 ሲሆን ሞዴሏም የተሰራው በ ’ ሲሆን ዲዛይኑ የተወሰደው ደሞ ከፈረስ መሆኑ እኔን ትንሽ አስገርሞኛል!፦ መኪናዋን ሳያት ደሞ አስቆኛልም ምንጭ[]    ▄ ▅ ▆\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️አንድ አባት የልጁን ህክምና ወጪ ለመሸፈን ኩላሊቱን ሸጦ አድኗት ልጁ ግን ለፍቅረኛዋ ብላ እራሷን አጠፋች ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️  የተባለ ሜክሲኮ ውስጥ የሞኖር የ17 አመት ልጅ ፍቅረኛው በፍቅር አንገቱን ስትነክሰው ሞተ።አስከሬኑ ሲመረመርም ፍቅረኛው ስትነክሰው ወደ አንጎሉ ሚሄደው ደም በመቋረጡ እንደሞተ ��ረጋግጧል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️ብራዚል ውስጥ ያሉ የግንባታ ሰራተኞች የእግረኛ መንገድ እየሰሩ የሚሰሩበት ቦታ ላይ የቆመ መኪና ነበር። መኪማው እንዲነሳ የመኪናውን ባለቤት ቢፈልጉትም ሊያገኙት ስላልቻሉ ፎቶው ላይ እንደምታዩት አርገውታል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️የመጀመሪያው የአሜሪካ ባንክ ዝርፊያ 162,821 የአሜሪካ ዶላር ሲሆን ዘራፊው የዘረፈውን ገንዘብ መልሶ እዛው ባንክ በማስቀመጡ ሊያዝ ችሏል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ��ውቁ ኖሯል❔ ✔️አንድ ሰውዬ ሙዚየም ውስጥ መነፅሩ ወድቆበት ረስቶ ሲሄድ ሙዚየም ውስጥ የሚገቡ ሰዎች መነፅሩ የጥበብ ስራ መስሏቸው ፎቶ ሲያነሱት ነበር ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️በደቡብ ሱዳን 4 ኣይን ያለው አዲስ እንሰሳ ተገኝቷል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️ሴቷ አንበሳ ከወንዱ አንበሳ ጋር ወሲብ ማረግ ፈልጋ ወንዱ እምቢ ካለ ሰቷ የብልቱን ፍሬዎች ትነክሰዋለች ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርናል ✔️በለንደን የአንዲት ሴት ሶፋ ላ��� ቁጭ ብላ  እያየች ሞተች። ከዚያም ከሞተች 4 አመታት ቡሃላ ው ክፍት እንደሆነ እሷም ሶፋ ላይ እንደተቀመጠች አፅሟ ተገኘ ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ◽️ይገርማል◽️ ✔️አንዱ ተማሪ አምቡላንስ ውስጥ የልብ ምቱ ቆሞ በቃ ሞቷል ከተባለ ቡሃላ ሆስፒታል ውስጥ በድንገት ነቃ። ከዛ ሃኪሞች ከሞት ቡሃላ ህይወት ስሜቱ እንዴት ነው ብለው ሲጠይቁት ሚገርም ነገር ተናገረ። ✔️ሞተ በተባለ ሰኣት መንፈሱ ከሞተው ሰውነቱ አጠገብ ተቀምጦ የሆነውን ነገር እንዳየ ተናግሮ አምቡላንስ ውስጥ የተፈጠረውን ነገር በሙሉ አንድም ሳያስቀር ነገራቸው እናም ትክክል ነበር። ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️በህልማቹ ምታዩአቸው ሰዎች በሙሉ በእውነተኛ ህይወታቹ ያያቹሃቸው ሰዎች ናቸው ምክንያቱም ጭንቅላታችን አዲስ ፊት መፍጠር አይችልም ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 565183 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 4062 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=92839259\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=422\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 555842 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=49878383\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 805318 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 555842\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 782962\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 782962 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=366282 obj=12.2206 num_tokens=1529475 num_tokens/piece=4.17568\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=309574 obj=10.5768 num_tokens=1530089 num_tokens/piece=4.94256\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=232151 obj=10.544 num_tokens=1600868 num_tokens/piece=6.8958\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=231957 obj=10.5309 num_tokens=1602335 num_tokens/piece=6.9079\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=173961 obj=10.5786 num_tokens=1701644 num_tokens/piece=9.78176\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=173957 obj=10.5673 num_tokens=1701907 num_tokens/piece=9.78349\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=130467 obj=10.6383 num_tokens=1804888 num_tokens/piece=13.8341\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=130467 obj=10.622 num_tokens=1804807 num_tokens/piece=13.8334\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=97850 obj=10.7237 num_tokens=1909519 num_tokens/piece=19.5148\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=97850 obj=10.7021 num_tokens=1909427 num_tokens/piece=19.5138\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=73387 obj=10.834 num_tokens=2016162 num_tokens/piece=27.473\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=73387 obj=10.8061 num_tokens=2016223 num_tokens/piece=27.4738\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=55040 obj=10.9725 num_tokens=2126657 num_tokens/piece=38.6384\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=55040 obj=10.9379 num_tokens=2126693 num_tokens/piece=38.639\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=41280 obj=11.1395 num_tokens=2237178 num_tokens/piece=54.1952\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=41280 obj=11.095 num_tokens=2237219 num_tokens/piece=54.1962\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=30960 obj=11.3392 num_tokens=2352183 num_tokens/piece=75.9749\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=30960 obj=11.2874 num_tokens=2352276 num_tokens/piece=75.9779\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23220 obj=11.5788 num_tokens=2475886 num_tokens/piece=106.627\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=23220 obj=11.5186 num_tokens=2476066 num_tokens/piece=106.635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17415 obj=11.8606 num_tokens=2597616 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17415 obj=11.7912 num_tokens=2598859 num_tokens/piece=149.231\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13061 obj=12.1894 num_tokens=2727046 num_tokens/piece=208.793\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13061 obj=12.1088 num_tokens=2727135 num_tokens/piece=208.8\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9795 obj=12.5551 num_tokens=2860281 num_tokens/piece=292.014\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9795 obj=12.4654 num_tokens=2860775 num_tokens/piece=292.065\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7346 obj=12.9592 num_tokens=2997825 num_tokens/piece=408.089\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7346 obj=12.8599 num_tokens=2997939 num_tokens/piece=408.105\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5509 obj=13.3992 num_tokens=3147287 num_tokens/piece=571.299\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5509 obj=13.2811 num_tokens=3147394 num_tokens/piece=571.319\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4131 obj=13.8636 num_tokens=3304554 num_tokens/piece=799.94\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4131 obj=13.7407 num_tokens=3304617 num_tokens/piece=799.956\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3098 obj=14.3987 num_tokens=3468345 num_tokens/piece=1119.54\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3098 obj=14.2597 num_tokens=3472148 num_tokens/piece=1120.77\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2323 obj=14.9438 num_tokens=3621927 num_tokens/piece=1559.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2323 obj=14.797 num_tokens=3621927 num_tokens/piece=1559.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9232 num_tokens=3653345 num_tokens/piece=1660.61\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=14.8879 num_tokens=3653353 num_tokens/piece=1660.62\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `TIKVAH.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
        "print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ሕኩታችን ለመስጠት\n"
          ]
        }
      ],
      "source": [
        "print(sp.decode_ids([460, 133, 774, 1276]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "4vHnQbBOltZo",
        "outputId": "9bb1ecaf-2883-494c-e34b-5616efac126a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000\n",
            "▁በአዲስ\n",
            "460\n",
            "0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ]
        }
      ],
      "source": [
        "# returns vocab size\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "# id <=> piece conversion\n",
        "print(sp.id_to_piece(460))\n",
        "print(sp.piece_to_id('▁በአዲስ'))\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv6EeC2Y2PE"
      },
      "source": [
        "## Loads model from byte stream\n",
        "\n",
        "Sentencepiece's model file is just a serialized [protocol buffer](https://developers.google.com/protocol-buffers/). We can instantiate sentencepiece processor from byte object with **load_from_serialized_proto** method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0Bdi9SuxYAud",
        "outputId": "b1566541-288e-4aa3-9c75-e494d0ab276a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-26 01:01:07.971834: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-26 01:01:08.871255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-26 01:01:08.871594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-26 01:01:08.967643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-26 01:01:09.057447: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-26 01:01:09.059167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-26 01:01:15.557608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assumes that m.model is stored in non-Posix file system.\n",
        "serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load_from_serialized_proto(serialized_model_proto)\n",
        "\n",
        "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imfPyYlVZmxz"
      },
      "source": [
        "## User defined and control symbols\n",
        "\n",
        "We can define special tokens (symbols) to tweak the DNN behavior through the tokens.   Typical examples are  [BERT](https://arxiv.org/abs/1810.04805)'s special symbols., e.g., [SEP] and [CLS].\n",
        "\n",
        "There are two types of special tokens:\n",
        "\n",
        "- **user defined symbols**: Always treated as one token in any context. These symbols can appear in the input sentence.\n",
        "- **control symbol**:  We only reserve ids for these tokens. Even if these tokens appear in the input text, they are not handled as one token. User needs to insert ids explicitly after encoding.\n",
        "\n",
        "For experimental purpose, user defined symbols are easier to use since user can change the behavior just by modifying the input text. However,  we want to use control symbols in the production setting in order to avoid users from tweaking the behavior by feeding these special symbols in their input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "dngckiPMcWbA",
        "outputId": "e52883b1-6452-4a90-8802-945c9ac9d5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<sep>', '▁ኤምባሲ', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=cleaned.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: cleaned.txt\n",
            "  input_format: \n",
            "  model_prefix: m_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <sep>\n",
            "  user_defined_symbols: <cls>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: cleaned.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5053 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: መልሱ ቀድሞ የመለሰው ●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ = 2 1⃣ =1 1⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 1⃣ = 1 1⃣   = 1 1⃣ §ä = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ = 2 2⃣ =1 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 2⃣ = 1 2⃣   = 1 2⃣ §ä = 1\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ = 2 2⃣ ß =1 2⃣ =1 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 1 2⃣ = 1 2⃣   = 1 2⃣ §ä = 1 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: የ ጥያቄ መላሾች ደረጃቸው 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3 2⃣ =3 2⃣  ⚽⚽ =3 2⃣ =3 2⃣ =3 2⃣ =3 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: 1⃣ =6 2⃣●▬▬๑▅▆▇█۩۩۩۩█▇▆▅●▬▬๑ = 3 2⃣ = 3 2⃣   = 3 2⃣ §ä = 3 2⃣  = 3 2⃣ = 3 2⃣ = 3 2⃣ # = 3 2⃣ = 3 2⃣ = 3 2⃣ = 3 2⃣ =3 2⃣  ⚽⚽ =3 2⃣ =3 2⃣ =3 2⃣ =3 ©4 3 3  ™  ♨️ ♦️ ♨️ ♦️\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️የአለማችን የመጀመሪያዋ መኪና የተሰራችው እንደ ፈረንጆቹ አቆጣጠር በ1885 ሲሆን ሞዴሏም የተሰራው በ ’ ሲሆን ዲዛይኑ የተወሰደው ደሞ ከፈረስ መሆኑ እኔን ትንሽ አስገርሞኛል!፦ መኪናዋን ሳያት ደሞ አስቆኛልም ምንጭ[]    ▄ ▅ ▆\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️አንድ አባት የልጁን ህክምና ወጪ ለመሸፈን ኩላሊቱን ሸጦ አድኗት ልጁ ግን ለፍቅረኛዋ ብላ እራሷን አጠፋች ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️  የተባለ ሜክሲኮ ውስጥ የሞኖር የ17 አመት ልጅ ፍቅረኛው በፍቅር አንገቱን ስትነክሰው ሞተ።አስከሬኑ ሲመረመርም ፍ���ረኛው ስትነክሰው ወደ አንጎሉ ሚሄደው ደም በመቋረጡ እንደሞተ ተረጋግጧል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርማል ✔️ብራዚል ውስጥ ያሉ የግንባታ ሰራተኞች የእግረኛ መንገድ እየሰሩ የሚሰሩበት ቦታ ላይ የቆመ መኪና ነበር። መኪማው እንዲነሳ የመኪናውን ባለቤት ቢፈልጉትም ሊያገኙት ስላልቻሉ ፎቶው ላይ እንደምታዩት አርገውታል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️የመጀመሪያው የአሜሪካ ባንክ ዝርፊያ 162,821 የአሜሪካ ዶላር ሲሆን ዘራፊው የዘረፈውን ገንዘብ መልሶ እዛው ባንክ በማስቀመጡ ሊያዝ ችሏል ምንጭ[] ▄ ▅ ▆ ��� ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️አንድ ሰውዬ ሙዚየም ውስጥ መነፅሩ ወድቆበት ረስቶ ሲሄድ ሙዚየም ውስጥ የሚገቡ ሰዎች መነፅሩ የጥበብ ስራ መስሏቸው ፎቶ ሲያነሱት ነበር ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️በደቡብ ሱዳን 4 ኣይን ያለው አዲስ እንሰሳ ተገኝቷል ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️ሴቷ አንበሳ ከወንዱ አንበሳ ጋር ወሲብ ማረግ ፈልጋ ወንዱ እምቢ ካለ ሰቷ የብልቱን ፍሬዎች ትነክሰዋለች ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ይገርናል ✔️በለንደን የአንዲት ሴት ሶፋ ላይ ቁጭ ብላ  እያየች ሞተች። ከዚያም ከሞተች 4 አመታት ቡሃላ ው ክፍት እንደሆነ እሷም ሶፋ ላይ እንደተቀመጠች አፅሟ ተገኘ ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ◽️ይገርማል◽️ ✔️አንዱ ተማሪ አምቡላንስ ውስጥ የልብ ምቱ ቆሞ በቃ ሞቷል ከተባለ ቡሃላ ሆስፒታል ውስጥ በድንገት ነቃ። ከዛ ሃኪሞች ከሞት ቡሃላ ህይወት ስሜቱ እንዴት ነው ብለው ሲጠይቁት ሚገርም ነገር ተናገረ። ✔️ሞተ በተባለ ሰኣት መንፈሱ ከሞተው ሰውነቱ አጠገብ ተቀምጦ የሆነውን ነገር እንዳየ ተናግሮ አምቡላንስ ውስጥ የተፈጠረውን ነገር በሙሉ አንድም ��ያስቀር ነገራቸው እናም ትክክል ነበር። ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(389) LOG(INFO) Reserved chars are found. Skipped: ❔ይህን ያውቁ ኖሯል❔ ✔️በህልማቹ ምታዩአቸው ሰዎች በሙሉ በእውነተኛ ህይወታቹ ያያቹሃቸው ሰዎች ናቸው ምክንያቱም ጭንቅላታችን አዲስ ፊት መፍጠር አይችልም ምንጭ[] ▄ ▅ ▆ ▇ ▇ ▆ ▅ ▄\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 565183 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 4062 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=92839259\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=422\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 555842 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=49878383\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 805318 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 555842\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 782962\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 782962 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=366282 obj=12.2206 num_tokens=1529475 num_tokens/piece=4.17568\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=309574 obj=10.5768 num_tokens=1530089 num_tokens/piece=4.94256\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=232151 obj=10.544 num_tokens=1600868 num_tokens/piece=6.8958\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=231957 obj=10.5309 num_tokens=1602335 num_tokens/piece=6.9079\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=173961 obj=10.5786 num_tokens=1701644 num_tokens/piece=9.78176\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=173957 obj=10.5673 num_tokens=1701907 num_tokens/piece=9.78349\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=130467 obj=10.6383 num_tokens=1804888 num_tokens/piece=13.8341\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=130467 obj=10.622 num_tokens=1804807 num_tokens/piece=13.8334\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=97850 obj=10.7237 num_tokens=1909519 num_tokens/piece=19.5148\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=97850 obj=10.7021 num_tokens=1909427 num_tokens/piece=19.5138\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=73387 obj=10.834 num_tokens=2016162 num_tokens/piece=27.473\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=73387 obj=10.8061 num_tokens=2016223 num_tokens/piece=27.4738\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=55040 obj=10.9725 num_tokens=2126657 num_tokens/piece=38.6384\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=55040 obj=10.9379 num_tokens=2126693 num_tokens/piece=38.639\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=41280 obj=11.1395 num_tokens=2237178 num_tokens/piece=54.1952\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=41280 obj=11.095 num_tokens=2237219 num_tokens/piece=54.1962\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=30960 obj=11.3392 num_tokens=2352183 num_tokens/piece=75.9749\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=30960 obj=11.2874 num_tokens=2352276 num_tokens/piece=75.9779\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23220 obj=11.5788 num_tokens=2475886 num_tokens/piece=106.627\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=23220 obj=11.5186 num_tokens=2476066 num_tokens/piece=106.635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17415 obj=11.8606 num_tokens=2597616 num_tokens/piece=149.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17415 obj=11.7912 num_tokens=2598859 num_tokens/piece=149.231\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13061 obj=12.1894 num_tokens=2727046 num_tokens/piece=208.793\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13061 obj=12.1088 num_tokens=2727135 num_tokens/piece=208.8\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9795 obj=12.5551 num_tokens=2860281 num_tokens/piece=292.014\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9795 obj=12.4654 num_tokens=2860775 num_tokens/piece=292.065\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7346 obj=12.9592 num_tokens=2997825 num_tokens/piece=408.089\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7346 obj=12.8599 num_tokens=2997939 num_tokens/piece=408.105\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5509 obj=13.3992 num_tokens=3147287 num_tokens/piece=571.299\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5509 obj=13.2811 num_tokens=3147394 num_tokens/piece=571.319\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4131 obj=13.8636 num_tokens=3304554 num_tokens/piece=799.94\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4131 obj=13.7407 num_tokens=3304617 num_tokens/piece=799.956\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3098 obj=14.3987 num_tokens=3468345 num_tokens/piece=1119.54\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3098 obj=14.2597 num_tokens=3472148 num_tokens/piece=1120.77\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2323 obj=14.9438 num_tokens=3621927 num_tokens/piece=1559.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2323 obj=14.797 num_tokens=3621927 num_tokens/piece=1559.16\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=14.9232 num_tokens=3653345 num_tokens/piece=1660.61\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=14.8879 num_tokens=3653353 num_tokens/piece=1660.62\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_user.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_user.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbol to apper in the text.\n",
        "print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "5awRJ0y1oYm-",
        "outputId": "a5fa1ef9-ee5f-4f7d-b6bd-b611979b7350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<', 's', 'e', 'p', '>', 'ኤ', 'ም', 'ባ', 'ሲ', '<', 'c', 'l', 's', '>']\n",
            "3\n",
            "4\n",
            "3= \n",
            "4= \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_ctrl\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  control_symbols: <sep>\n",
            "  control_symbols: <cls>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_ctrl.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_ctrl.vocab\n"
          ]
        }
      ],
      "source": [
        "# Example of control symbols\n",
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_ctrl --control_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_ctrl = spm.SentencePieceProcessor()\n",
        "sp_ctrl.load('m_ctrl.model')\n",
        "\n",
        "# control symbols just reserve ids.\n",
        "print(sp_ctrl.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep>ኤምባሲ<cls>'))\n",
        "print(sp_ctrl.piece_to_id('<sep>'))  # 3\n",
        "print(sp_ctrl.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_ctrl.decode_ids([3]))  # decoded to empty\n",
        "print('4=', sp_ctrl.decode_ids([4]))  # decoded to empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ppZck91s0rq"
      },
      "source": [
        " BOS/EOS (&lt;s&gt;, &lt;/s&gt;) are defined as control symbols, but we can define them as user defined symbols."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "PQoZ8paVhcEL",
        "outputId": "c17d2ae2-17cd-4875-997c-fe66dba23dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', '<', 's', '>', '▁በአዲስ', '<', '/', 's', '>']\n",
            "['▁', '<s>', '▁በአዲስ', '</s>']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bos_as_user\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: <s>\n",
            "  user_defined_symbols: </s>\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_bos_as_user.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_bos_as_user.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are segmented. (default behavior)\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m_bos_as_user.model')\n",
        "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are handled as one token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ2GjO5Tmjk9"
      },
      "source": [
        "## Manipulating BOS/EOS/EOS/PAD symbols\n",
        "\n",
        "BOS, EOS, UNK, and PAD ids can be obtained with **bos_id()**, **eos_id()**,  **unk_id()**, and **pad_id()** methods. We can explicitly insert these ids as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "UtFQqK3tmp7G",
        "outputId": "03e82bec-be40-4574-ab62-c66cdc3f28a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bos= 1\n",
            "eos= 2\n",
            "unk= 0\n",
            "pad= -1\n",
            "[434, 111]\n",
            "[1, 434, 111, 2]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "print('bos=', sp.bos_id())\n",
        "print('eos=', sp.eos_id())\n",
        "print('unk=', sp.unk_id())\n",
        "print('pad=', sp.pad_id())  # disabled by default\n",
        "\n",
        "\n",
        "print(sp.encode_as_ids('በአዲስ አበባ'))\n",
        "\n",
        "# Prepend or append bos/eos ids.\n",
        "print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDXA3Q6kjCS"
      },
      "source": [
        "## Sampling and nbest segmentation for subword regularization\n",
        "\n",
        "When **--model_type=unigram** (default) is used,  we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "nSQp93qflZO3",
        "outputId": "d5b45b62-2789-4879-b80b-f441e8b594af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁በአዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በ', 'አዲስ', '▁አበባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በአዲስ', '▁አበባ']\n",
            "['▁በ', 'አ', 'ዲ', 'ስ', '▁አ', 'በ', 'ባ']\n",
            "['▁', 'በ', 'አዲስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአ', 'ዲ', 'ስ', '▁አ', 'በ', 'ባ']\n",
            "['▁በአ', 'ዲ', 'ስ', '▁አበባ']\n",
            "[434, 111]\n",
            "[434, 111]\n",
            "[8, 1066, 111]\n",
            "[8, 1066, 111]\n",
            "[3, 34, 1066, 111]\n",
            "[8, 130, 110, 13, 16, 34, 41]\n",
            "[434, 3, 130, 34, 41]\n",
            "[434, 111]\n",
            "[3, 34, 1066, 3, 130, 34, 41]\n",
            "[8, 1066, 111]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# Can obtain different segmentations per request.\n",
        "# There are two hyperparamenters for sampling (nbest_size and inverse temperature). see the paper [kudo18] for detail.\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('በአዲስ አበባ', -1, 0.1))\n",
        "\n",
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('በአዲስ አበባ', -1, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9V1snUZdlb_v",
        "outputId": "bae2ca99-aca3-4013-c240-53b09a0bb684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['▁በአዲስ', '▁አበባ'], ['▁በ', 'አዲስ', '▁አበባ'], ['▁በአዲስ', '▁አ', 'በ', 'ባ'], ['▁', 'በ', 'አዲስ', '▁አበባ'], ['▁በአ', 'ዲ', 'ስ', '▁አበባ'], ['▁በአዲስ', '▁', 'አ', 'በ', 'ባ'], ['▁በ', 'አ', 'ዲ', 'ስ', '▁አበባ'], ['▁በ', 'አዲስ', '▁አ', 'በ', 'ባ'], ['▁', 'በ', 'አ', 'ዲ', 'ስ', '▁አበባ'], ['▁', 'በ', 'አዲስ', '▁አ', 'በ', 'ባ']]\n",
            "[[434, 111], [8, 1066, 111], [434, 16, 34, 41], [3, 34, 1066, 111], [188, 110, 13, 111], [434, 3, 130, 34, 41], [8, 130, 110, 13, 111], [8, 1066, 16, 34, 41], [3, 34, 130, 110, 13, 111], [3, 34, 1066, 16, 34, 41]]\n"
          ]
        }
      ],
      "source": [
        "# get 10 best\n",
        "print(sp.nbest_encode_as_pieces('በአዲስ አበባ', 10))\n",
        "print(sp.nbest_encode_as_ids('በአዲስ አበባ', 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH6cxuVNcDKh"
      },
      "source": [
        "## BPE (Byte pair encoding) model\n",
        "\n",
        "Sentencepiece supports BPE (byte-pair-encoding) for subword segmentation with **--model_type=bpe** flag.   We do not find empirical differences in translation quality between BPE and unigram model, but unigram model can perform sampling and n-best segmentation. See subword regularization paper [[kudo18]](https://www.google.com/search?q=subword+regularization&rlz=1CAASUL_enJP841&oq=subword+regu&aqs=chrome.0.69i59j69i61j69i57j69i61l2j0.1571j0j7&sourceid=chrome&ie=UTF-8) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MNQxuX4Mc0KY",
        "outputId": "a6ed3e99-46c3-4c5e-dc8e-80394e17e363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** BPE ***\n",
            "['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አ', 'ሜሪካ', 'ኤ', 'ምባ', 'ሲ']\n",
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_bpe\n",
            "  model_type: BPE\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=105325 min_freq=791\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13879 size=20 all=20266 active=3280 piece=▁ሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7808 size=40 all=21877 active=4891 piece=▁ክ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6491 size=60 all=23393 active=6407 piece=ጵያ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4963 size=80 all=24730 active=7744 piece=▁ኮ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4087 size=100 all=25905 active=8919 piece=▁አካ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4072 min_freq=538\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3567 size=120 all=26970 active=2336 piece=▁ዶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3272 size=140 all=28072 active=3438 piece=▁ሳ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2959 size=160 all=29207 active=4573 piece=▁አስተ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2689 size=180 all=30138 active=5504 piece=ሊስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2412 size=200 all=31078 active=6444 piece=▁ቀን\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2410 min_freq=413\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2294 size=220 all=32106 active=2564 piece=▁V\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2122 size=240 all=33074 active=3532 piece=▁ተና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1993 size=260 all=34129 active=4587 piece=▁አለ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1854 size=280 all=34963 active=5421 piece=ተው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1735 size=300 all=36013 active=6471 piece=▁ገልፀ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1733 min_freq=327\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1655 size=320 all=36636 active=2415 piece=ሰባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1564 size=340 all=37391 active=3170 piece=ስቲ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1495 size=360 all=38164 active=3943 piece=▁ኣም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1415 size=380 all=38820 active=4599 piece=▁በዚህ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1341 size=400 all=39698 active=5477 piece=ላለ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1335 min_freq=282\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1279 size=420 all=40472 active=2661 piece=▁ዋና\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1237 size=440 all=41149 active=3338 piece=ማር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1199 size=460 all=41991 active=4180 piece=▁ሽ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1131 size=480 all=42726 active=4915 piece=▁ሆነ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1080 size=500 all=43463 active=5652 piece=▁ቃ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1077 min_freq=247\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1038 size=520 all=44186 active=2861 piece=▁ሶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1005 size=540 all=44802 active=3477 piece=▁ወይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=959 size=560 all=45284 active=3959 piece=መር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=932 size=580 all=46043 active=4718 piece=ከያ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=910 size=600 all=46579 active=5254 piece=▁ሀገር\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=905 min_freq=219\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=880 size=620 all=47114 active=2854 piece=VAH\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=859 size=640 all=47743 active=3483 piece=ሜሪካ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=838 size=660 all=48462 active=4202 piece=▁ይገባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=809 size=680 all=49075 active=4815 piece=ላንት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=785 size=700 all=49969 active=5709 piece=▁ፈተና\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=784 min_freq=193\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=765 size=720 all=50375 active=2887 piece=በው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=744 size=740 all=51211 active=3723 piece=▁ግንባ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=725 size=760 all=51709 active=4221 piece=ጣት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=706 size=780 all=52325 active=4837 piece=▁የሚያስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=688 size=800 all=52989 active=5501 piece=ስፒ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=688 min_freq=172\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=672 size=820 all=53709 active=3359 piece=ገለፁ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=659 size=840 all=54359 active=4009 piece=ብሄ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=647 size=860 all=54842 active=4492 piece=ህል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=634 size=880 all=55517 active=5167 piece=ቅረ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=623 size=900 all=56137 active=5787 piece=ጠት\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=622 min_freq=156\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=614 size=920 all=56484 active=3140 piece=ስፋ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=600 size=940 all=57146 active=3802 piece=ኳን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=586 size=960 all=57616 active=4272 piece=▁ዝግጅት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=576 size=980 all=58091 active=4747 piece=▁ከሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=561 size=1000 all=58865 active=5521 piece=▁አፈ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=561 min_freq=142\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=553 size=1020 all=59506 active=3560 piece=▁መጠ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=540 size=1040 all=59904 active=3958 piece=▁ፕሬዚ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=529 size=1060 all=60242 active=4296 piece=ራቸው\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=519 size=1080 all=60893 active=4947 piece=▁ቤተሰ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=510 size=1100 all=61396 active=5450 piece=ግብ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=510 min_freq=131\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=503 size=1120 all=61832 active=3450 piece=▁የሃ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=491 size=1140 all=62322 active=3940 piece=▁በከተማ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=480 size=1160 all=62924 active=4542 piece=ማን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=1180 all=63555 active=5173 piece=▁በዚህም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=463 size=1200 all=63955 active=5573 piece=▁መም\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=462 min_freq=122\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=455 size=1220 all=64555 active=3783 piece=▁ዳይሬክተር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=446 size=1240 all=65184 active=4412 piece=▁ምላሽ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=439 size=1260 all=65672 active=4900 piece=▁የፈ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=433 size=1280 all=66087 active=5315 piece=ሄድ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=426 size=1300 all=66644 active=5872 piece=▁ከተሞች\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=426 min_freq=113\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=419 size=1320 all=67173 active=3857 piece=ሬሽን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=412 size=1340 all=67622 active=4306 piece=ሜን\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=405 size=1360 all=68044 active=4728 piece=▁ደስ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=399 size=1380 all=68609 active=5293 piece=ያዝ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=391 size=1400 all=69089 active=5773 piece=አዴግ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=391 min_freq=105\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=384 size=1420 all=69647 active=4005 piece=ረር\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=378 size=1440 all=70193 active=4551 piece=▁የሶ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=373 size=1460 all=70656 active=5014 piece=ኛውም\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=1480 all=71142 active=5500 piece=▁በበኩ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=363 size=1500 all=71624 active=5982 piece=▁እየተካሄደ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=362 min_freq=98\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=357 size=1520 all=72040 active=3991 piece=▁እንዳል\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=351 size=1540 all=72411 active=4362 piece=ገነ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=345 size=1560 all=72966 active=4917 piece=ሳሳይ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=341 size=1580 all=73472 active=5423 piece=ንደ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=337 size=1600 all=73995 active=5946 piece=ለጠ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=337 min_freq=92\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=332 size=1620 all=74270 active=3926 piece=▁ኣመታት\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=328 size=1640 all=74809 active=4465 piece=▁ጊ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=322 size=1660 all=75210 active=4866 piece=▁ወይዘ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=318 size=1680 all=75502 active=5158 piece=ብን\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_bpe.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_bpe.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=cleaned.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))\n",
        "print(sp_bpe.nbest_encode_as_pieces('በአዲስ አበባ', 5))  # returns an empty list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "EZrj1zCkvK8v",
        "outputId": "8d5421d9-9b89-440d-c91a-f03c081947d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Unigram ***\n",
            "['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ']\n",
            "[['▁በአዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በ', 'አዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁', 'በ', 'አዲስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በአ', 'ዲ', 'ስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ'], ['▁በ', 'አ', 'ዲ', 'ስ', 'አ', 'በ', 'ባ', 'የ', 'አሜሪካ', 'ኤ', 'ም', 'ባ', 'ሲ']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: TIKVAH.txt\n",
            "  input_format: \n",
            "  model_prefix: m_unigram\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
            "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
            "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
            "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
            "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
            "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
            "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
            "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
            "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
            "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
            "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
            "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
            "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
            "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
            "trainer_interface.cc(686) LOG(INFO) Saving model: m_unigram.model\n",
            "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_unigram.vocab\n"
          ]
        }
      ],
      "source": [
        "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram')\n",
        "sp_unigram = spm.SentencePieceProcessor()\n",
        "sp_unigram.load('m_unigram.model')\n",
        "\n",
        "print('*** Unigram ***')\n",
        "print(sp_unigram.encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ'))\n",
        "print(sp_unigram.nbest_encode_as_pieces('በአዲስአበባየአሜሪካኤምባሲ', 5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
